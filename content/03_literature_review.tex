\section{Literature Review}
\label{sec:literature}

This section provides an overview of artificial intelligence (AI), including hybrid intelligence, adaptive AI,
design patterns for hybrid intelligent systems, and how the use of such systems in enterprises can create a
competitive advantage.

\subsection{Overview of Artificial Intelligence}

AI involves the creation of computer programs and algorithms that allow machines to replicate human cognition and behavior,
which includes the capabilities of perception, learning, reasoning, solving problems, and making decisions. AI can be broadly
subdivided into symbolic and sub-symbolic approaches, see e.g., \cite{eliasmithSymbolicSubsymbolic2006}. Symbolic approaches
involve the use of explicit symbols and rules to represent knowledge and reason in a way that is easily understood and explainable
by humans, while sub-symbolic (or \textit{connectionist}) approaches aim to learn complex patterns from vast amounts of data
using neural networks \citep{ilkouSymbolicVsSubsymbolic2020}. Hybrid AI refers to systems combining symbolic and sub-symbolic
approaches. Hybrid AI systems can be anywhere from loosely coupled to tightly integrated \citep{garcezNeurosymbolicAI3rd2023}.

Loosely coupled hybrid AI systems typically involve a human, which is also known as \textit{human in the loop} (HITL)
computing. In such systems the humans and AI work together towards common goals, augmenting the human intellect and
overcoming human limitations and cognitive biases \citep{akataResearchAgendaHybrid2020}. \cite{akataResearchAgendaHybrid2020}
refer to this combination of human and machine intelligence as \textit{hybrid intelligence}. The concept is also known as
intelligence augmentation and amplification, see e.g., \cite{schmidtAugmentingHumanIntellect2017} and
\cite{zhouIntelligenceAugmentationBuilding2021}. The idea of hybrid intelligence can be traced back to Joseph Licklider's
``man-computer symbiosis'' and Douglas Engelbart's vision of increasing human capabilities trough better and faster machine
understanding \citep[and references cited therein]{schmidtAugmentingHumanIntellect2017}. With the advanced and ubiquitous
digital technologies now available, hybrid intelligent systems show the potential for improving the outcomes of AI systems, 
hence \textit{augmenting} rather than replacing human intelligence \citep{schmidtAugmentingHumanIntellect2017,akataResearchAgendaHybrid2020}.
Hybrid intelligent systems have recently received attention from the leaders in the field as a response to an ever increased
focus on sub-symbolic approaches and deep learning in particular: \cite{kambhampatiChallengesHumanAwareAI2020} demanded that
AI researchers build human-aware AI systems that work synergistically with humans, including considering the human mental state,
recognizing desires and intentions, and providing proactive support to humans. In particular, AI researchers should aim
at systems that show the capabilities of \textit{explicability} (AI agents should show behavior that is expected by
humans) and \textit{explainability} (AI agents -- if behaving unexpectedly -- should be able to provide an explanation)
\citep{kambhampatiChallengesHumanAwareAI2020}. Such human-aware AI systems act as a human collaborator and must \textit{"sense,
understand, and react to a wide range of complex human behavioral qualities, like attention, motivation, emotion, creativity,
planning, or argumentation"} \citep{kortelingHumanArtificialIntelligence2021}.

To reach human-level intelligence, some argued that AI systems would need to be \textit{degraded} at some point due to the
dissimilar nature of human and machine intelligence \citep{kortelingHumanArtificialIntelligence2021}.
The physical substrate (biological, respectively digital) determines the cognitive abilities and limitations of human
\textit{versus} artificial intelligence, with human cognitive faculties being limited by the biological and evolutionary
origin of intelligence \citep{kortelingHumanArtificialIntelligence2021}. Further, \cite{kortelingHumanArtificialIntelligence2021}
argue that the pursuit of Artificial General Intelligence (AGI), i.e., machines reaching human-level intelligence, may
be a misleading goal due to these limitations of human intelligence. They conclude that (hybrid intelligent) AI systems
supporting human decision-making appear to be the best way forward for implementing better solutions, even if this means
that we stick to narrow AI applications for the foreseeable future \citep{kortelingHumanArtificialIntelligence2021}.

The concept of narrow AI refers to AI applications that have been trained with specific data for narrowly defined use
cases, typically yielding good performances on a single, predefined task. Hence, narrow AI applications typically lack
versatility: due to the limited amount and variety of training data, changing the use case of the AI typically
requires re-training a new model with different training data. On the other side, narrow AI application require fewer
data points and compute time for training and may thus be re-trained more frequently or continuously trained on new data
(i.e., online training). Further, narrow AI models have a smaller number of parameters (i.e., weights, biases) and
thus also require less compute time and resources at inference time.

In contrast, broad AI applications such as large language models (LLMs) are sophisticated systems that successfully
adapt to different cognitive tasks by virtue of their sensory perception, computational learning, and
previous experience \citep{hochreiterBroadAI2022}. LLMs were originally designed as large neural networks trained
on vast amounts of textual data collected from the Internet. Recently a number of such large models were trained
with multimodal data, including text, images, speech, and video \citep{bommasaniOpportunitiesRisksFoundation2022}.
The resulting broad AI models are good at a wide variety of tasks with the performance being often close to that of
specialized narrow AI models \citep{bommasaniOpportunitiesRisksFoundation2022}. Surprisingly, as LLMs became larger,
they also exhibited an increasing number of emergent capabilities that were unpredictable and absent in smaller models
\citep{weiEmergentAbilitiesLarge2022}. This has sparked enormous interest in LLMs as potentially a single AI model 
can be adapted to a wide variety of use cases. 

However, even the broadest of current LLMs show severe limitations, such
as hallucination \citep{jiSurveyHallucinationNatural2023}, shortcomings in their capability to reason 
\citep{bangMultitaskMultilingualMultimodal2023}, or biases \citep{tamkinUnderstandingCapabilitiesLimitations2021}.
Thus, the term \textit{foundation model} was proposed by researchers at the Human-centered AI (HAI) institute of the
Stanford University to better reflect the nature of the multimodal training data and the severe limitations that remain
in these models \citep{bommasaniOpportunitiesRisksFoundation2022}. One famous example of a foundation model is GPT, which
has been popularized through a chatbot user interface as ChatGPT. ChatGPT is arguably highly interactive, as the user has
to prompt the AI model. Additionally, ChatGPT and other
foundation model exhibit the emerging capability of \textit{in-context learning}, meaning they can learn from a
small set of examples in the prompt, and apply that context to generate more precise and succinct responses to
user's prompt \citep{bommasaniOpportunitiesRisksFoundation2022}. The capability of in-context learning further
reduces adoption obstacles for humans and enables the use of generative AI models in diverse downstream tasks
\citep{bommasaniOpportunitiesRisksFoundation2022}.

\subsection{Adaptive Artificial Intelligence}

There are several aspects of adaptability of AI. First, AI systems may need to adapt to various tasks. Second,
AI systems may need to adapt to a human teammate in the course of completing a task that is jointly performed by
an AI-human team. Third, AI systems may need to adapt to varying needs of different users. Fourth, AI systems
may need to adapt to other autonomous or human agents in the environment. Finally, AI systems may need to adapt
to changes of the environment.

\textit{Various Tasks}: 

\textit{Joint Task Completion}:
As AI becomes ubiquitous, teams are exploring ways to integrate AI agents and robots in their work towards
achieving common goals. \cite{zhaoRoleAdaptationCollective2022} distinguish four ways how the AI agents may need
to adapt in the context of a team: (1) adaption to goals and intentions of the human teammates; (2) adaptation
to cognitive features of the human; (3) adaptation to physical factors of the human in robot-human interactions
(e.g., fatigue of the human); and (4) adaptation of learned human models to transfer a learned model to the
interaction with another human.

\cite{hauptmanAdaptOvercomePerceptions2023} propose that the role and contribution of AI agents 


\textit{Different User Needs}:
\cite{kabudiAIenabledAdaptiveLearning2021} reviewed the literature regarding learning systems used in education
that utilize some form of AI that adapts to the user. 

\textit{Interactions with Agents}:
\cite{madeiraDesigningReinforcementLearningBased2006} have researched the adaptability of AI 
systems as part of strategy games: the reward functions in reinforcement learning algorithms can be designed to consider
the effect of their behavior on other participating agents \citep{madeiraDesigningReinforcementLearningBased2006}.
In a single-agent reinforcement learning setting, the agent is trained based on its actions and the state of the
environment. In a multiagent reinforcement learning (MARL) setting, the agent is trained based on the effect of its
actions on the environment while consider potential (re)actions of the other agents \citep{caneseMultiAgentReinforcementLearning2021}.
AI models trained through MARL thus show a high adaptability to other agents in the environment. However, MARL applications
face challenges which make them hard to use in real-world applications, including limitations in the scalability to settings
with large numbers of agents, and the non-stationarity of the environment \citep{caneseMultiAgentReinforcementLearning2021}. 
Further, AI in hybrid intelligent systems may also need to adapt to a variety of temporal changes occurring in the environment
and in the composition of the group of agents \citep{akataResearchAgendaHybrid2020}. Nevertheless, strategy games with a
multitude of agents working towards a common goal are an interesting subject as they share some commonalities with enterprises:
a group of agents is interacting, and each agent is taking decisions towards reaching a common goal.

\textit{Changing Environment}: changes in the environment that requires adaptation of an AI system could
include societal trends, political or legal changes, etc. To adapt to such changes an AI system can be retrained
with newer training data. A common problem
observed with machine learning algorithms subject to periodic or continuous retraining (i.e., online training)
is model drift: the quality of new data may worsen over time or the distribution of the new data may shift over
time, slowly degrading the performance of the machine learning model \citep{nelsonEvaluatingModelDrift2015}. Thus,
while retraining is needed to adapt the AI system to changes in the environment, great care has to be taken to 
train and evaluate the model with current and high quality data.   



Adaptive AI: AI can adapt to user's specific needs. This can be part of a recommender system or a decision support
system that adapts according to the input received by users. As such, hybrid AI systems that are loosely coupled can 
be seen as adaptive AI systems. On the other hand, adaptive AI can also mean that the AI adapts to changes in the
environment, be it new data (adapting the AI to new data, i.e., retraining with new or more data or continuous online
learning of the AI model) or new use cases (adapting the task of the AI model).

Safety of adaptive AI: it may learn and apply wrong strategies, quality of the incoming data stream may degrade 
over time, hence degrading the model's performance. Such degrading can be slowly happening over time, so that it 
is difficult to detect.

Further, highly adaptive systems pose a threat to the explainability of the system, as today's decision from 
the system might be totally different from the yesterday's decisions, making it hard for humans to understand
how the system works.

\subsubsection{Hybrid Intelligent Approaches Involving Foundation Models}

\begin{itemize}
    \item Agents 
    \item Mixed architecture, e.g., MRKL \citep{@karpasMRKLSystemsModular2022}
    \item Using the model as IR agent 
\end{itemize}

\subsection{Design Principles for Hybrid Intelligent Systems}

Hybrid AI systems can be represented by a boxology notation with common design patterns \citep{harmelenBoxologyDesignPatterns2019,
vanbekkumModularDesignPatterns2021,witschelVisualizationPatternsHybrid2021}.

\cite{ostheimerAllianceHumansMachines2021} developed a framework of eight principles for the design of human-in-the-loop (HITL) 
computing. They argue that such hybrid systems achieve higher accuracy and reliability of machine learning algorithms. Using a 
case in the manufacturing industry, they showed that the efficiency of operational processes could be increased by applying an 
algorithm that followed these design principles \citep{ostheimerAllianceHumansMachines2021}.

% box with HITL design principles
{
    \begin{center}
        \vskip 0.2in
        \fcolorbox{gray}{white}{
        \parbox{0.85\textwidth}{
        \textbf{Box 1. HITL Computing Design principles \citep{ostheimerAllianceHumansMachines2021}.}
        \begin{enumerate}
            \item Principle of client-designer relationship: designers should aim for mutual knowledge exchange with clients to foster the understanding
                    of which aspects of a system are influenced by human or artificial intelligence.
            \item Principle of sustainable design: designers should keep up to date with the latest progress in the field of AI and apply the latest and
                    lasting AI techniques.
            \item Principle of extended vision
            \item Principle of AI-readiness
            \item Principle of hybrid intelligence
            \item Principle of use-case marketing
            \item Principle of power relationship
            \item Principle of human-AI trust
        \end{enumerate}}}
        \vskip 0.2in
    \end{center}
}


\subsection{Types of Hybrid Intelligent Systems}

\begin{itemize}
    \item Expert systems 
    \item Decision support systems
    \item Recommender algorithms with human decision-making
    \item Case-based reason systems
\end{itemize}

\subsection{Enterprise Competitiveness}

{\color{purple} @todo: what are the aspects of and factors increasing the competitiveness of enterprises?}


\subsection{Competitive Advantage Through AI}

\cite{xuCanArtificialIntelligence2021} found that post COVID-19 companies using AI in their products grew
faster than their peers. However, they could not observe evidence of the same effect before COVID-19, indicating
that this development is either very recent or was fueled by the COVID crisis. More recently
\cite{hoArtificialIntelligenceFirm2022} reviewed the potential benefits of AI for enterprises as reported
by selected previous studies published between 2016 and 2021:

\begin{itemize}
    \item reduced costs
    \item improved performance
    \item better decision-making
    \item higher customer satisfaction
    \item better customer segmentation
    \item improved customer experience
    \item better products \& services
    \item business innovation
\end{itemize}

Further, \cite{hoArtificialIntelligenceFirm2022} identified several empirical studies that reported a positive,
neutral or negative effect of AI on enterprise performance. In particular one study by ...liu et al. (2022)...
and cited in \cite{hoArtificialIntelligenceFirm2022} reported negative performance of AI-related adoption 
announcements on firm market value for 62 listed US companies between 2015-2019.
